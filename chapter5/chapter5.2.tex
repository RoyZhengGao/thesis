\section{BDS Model} \label{bds}

\begin{figure*} 
	% \advance\leftskip-1cm 
	\centering
	\includegraphics[width=1\textwidth]{img/chapter5/pipeline.pdf}
	\caption{ The overall architecture of BDS model in the $t_{th} $ time period. Each color refers to an individual model component. Dynamic Product Behavior Representation and Sentiment Prediction Pretraining are optimized before training the main multi-task model (Neighbour Product Selection \& Summarization Generation). Red dashed lines show the workflows to calculate RL reward and in return to optimize RL policy. Data Prerequisite steps are omitted here for simplicity but are illustrated in detail in the main paper. }
	\label{fig:c5_pipeline}
\end{figure*}

The overall model architecture is sketched in Figure \ref{fig:c5_pipeline}. My final goal is to generate product dynamic aspect summarizations using behavior data instead of product reviews. To optimize BDS model, the involved data are categorized into two groups: training \& validation data and seed product data. The training \& validation data contains products with both sufficient user behavior and temporal reviews in an individual time period. While seed product data are products required to have sufficient reviews across all time periods. The training \& validation data helps to calculate dynamic product behavior presentations (\textit{Section \ref{sc:dpbr}}) and pretrain product sentiment prediction model (\textit{Section \ref{sc:spp}}). Subsequently, a multi-task model dynamically selects neighbour products (\textit{Section \ref{sc:rns}}) from seed products whose sentimental phrases filtered from reviews  contribute to generate product aspect summarization (\textit{Section \ref{sc:sgt}}). 

\subsection{Data Prerequisite} \label{sc:dp}

Before optimizing BDS model, four following types of information need to be clarified and extracted as data prerequisite:

\textbf{Product Sentiment Distribution:} AliNLP\footnote{English Tutorial: https://www.alibabacloud.com/help/product/57736.html \\ Chinese Tutorial: https://data.aliyun.com/product/nlp}, a paid NLP service by Alibaba, is particularly developed to analyze the E-commerce reviews' sentiment polarity on aspects such as `\textit{Quality}' or `\textit{Fitness}'. I use its sentiment analyzer to label and rate the sentiment of a review towards a specific aspect as `None (0.5)', `Negative (0)' or `Positive (1)'. Each aspect sentiment score of a given product is calculated as the weighted average score of all three labels appeared in its eligible reviews (with more than ten words). Product sentiment distribution is a vector of all aspect sentiment scores normalized by dividing its sum, which is regarded as the ground truth for Sentiment Prediction Pretraining (Section \ref{sc:spp}). In this paper, four aspects are selected to construct product sentiment distribution, which will be illustrated in the Experiments (Section \ref{sc:exp}).

\textbf{Product Descriptive Phrase:} Each product is associated with a description profile. After applying AliNLP Named Entity Recognizer (NER) on their profiles, I only keep the noun phrases to characterize related products.

\textbf{Review Sentimental Phrase:} The AliNLP sentiment analyzer can also extract sentimental phrases related to specific aspects from seed product reviews, i.e., `poor quality' is a sentimental phrases of `\textit{Quality}' aspect. Later on, these filtered sentimental phrases will be concatenated with product descriptive phrases as the model input for aspect summarization (Section \ref{sc:sgt}).

\textbf{Seed Products:} The top products having the most sufficient reviews across all $t$ time periods are regarded as seed products. Their aspect sentiment distributions are pre-calculated directly from reviews in each time period, which are later used to guide neighbour product selection (Section \ref{sc:npst}). 

\subsection{Dynamic Product Behavior Representation} \label{sc:dpbr}

Matrix factorization is utilized on dynamic user behaviors to learn product behavior representation across all time periods. In the initial time period, I learn both user and product $m$ types of behavior representations via related user behavior $\{B^{(0)}_{1},...,B^{(0)}_{m}\}$, where a behavior type can refer to `\textit{Click}' or `\textit{Purchase}', etc. The $i_{th}$ type of user behavior $B^{(0)}_{i}$ is in a format of sparse matrix where each row denotes a user and each column denotes a product. The data points in $B^{(0)}_{i}$ denote users' $i_{th}$ type of behaviors on products. Considering all potential methods listed in \textit{Section \ref{sc:baseline}}, I empirically select Probabilistic Matrix Factorization (PMF) to decompose $B^{(0)}_{i}$ to user representation matrix $U^{(0)}_{i}$ and product representation matrix $P^{(0)}_{i}$ by satisfying the following equation:
\begin{equation}
B^{(0)}_{i} = U^{(0)}_{i} (P^{(0)}_{i})^\intercal, i = 1,...,m
\end{equation}

As user shopping preference is consistently stable, once user representation $U^{(0)}_{i}$ is calculated from the initial period, it remains unchanged and bridges product dynamic behavior representation in subsequent time periods. 

In the $t_{th}$ time period, the matrix form of Least Squares Approximation helps to calculate the $i_{th}$ type of product behavior representation $P^{(t)}_{i}$ given related user behavior $B^{(t)}_{i}$ and user consistent representation $U^{(0)}_{i}$:
 
\begin{equation}
\begin{aligned} 
& B^{(t)}_{i} = U^{(0)}_{i} (P^{(t)}_{i})^\intercal, i = 1,...,m \\
& \Rightarrow (U^{(0)}_{i})^\intercal (B^{(t)}_{i} - U^{(0)}_{i} (P^{(t)}_{i})^\intercal) = 0 \\
& \Rightarrow (U^{(0)}_{i})^\intercal U^{(0)}_{i} (P^{(t)}_{i})^\intercal = (U^{(0)}_{i})^\intercal B^{(t)}_{i} \\ 
& \Rightarrow (P^{(t)}_{i})^\intercal = (( U^{(0)}_{i})^\intercal U^{(0)}_{i})^{-1}(U^{(0)}_{i})^\intercal B^{(t)}_{i} \\
& \Rightarrow P_{i}^{(t)} = (B_{i}^{(t)})^\intercal U^{(0)}_{i}(( U^{(0)}_{i})^\intercal U^{(0)}_{i})^{-1}
\end{aligned}
\end{equation}


\subsection{Neighbour Product Selection Task}\label{sc:npst}

\subsubsection{Sentiment Prediction Pretraining} \label{sc:spp}

In the $t_{th}$ time period, I combine product behavior representation and category to estimate product aspect sentiment distribution via a hybrid CNN-MLP (Convolutional Neural Network and Multilayer Perceptron) approach. Let $c$ be the category of a target product, stored originally as one-hot embedding. To better represent its enriched information, I first apply an one-layer MLP with $\rm tahn(\cdot)$ activation function to convert it as a dense vector $v^{c}$.

\begin{equation}
v^{c} = {\rm tahn}(W^{c}c+b^{c})
\end{equation}

where $W^{c}$ and $b^c$ denote the weight matrix and bias respectively.

For the same product, I can also obtain its multi-type behavior representation $\{p^{(t)}_{1},...,p^{(t)}_{m}\}$, where $p^{(t)}_{m} \in P^{(t)}_{m}$ denotes its $m_{th}$ type behavior representation. As CNN kernel can help to filter out the most important dimensions (local features) from a vector, a CNN layer ${\rm cnn}(\cdot)$ with Max pooling mechanism ${\rm max\_pooling}(\cdot)$ is applied to capture product $i_{th}$ type local feature representation $l^{(t)}_{i}$.
\begin{equation}
% \vspace{-0.5em}
\begin{aligned} 
&l^{(t)}_{i} = {\rm max\_pooling}({\rm tahn}({\rm cnn}(p^{(t)}_{i}))), i= 1,...,m
\end{aligned} 
\end{equation}


Subsequently, I average all $m$ types of local feature representation to a single vector $l^{(t)}$. Similarly, I calculate the average product behavior vector $p^{(t)}$ as global feature representation of $m$ types of behaviors: 
\begin{equation}
\begin{aligned} 
&l^{(t)} = \frac{1}{m}\sum_{i = 1}^{m}l^{(t)}_{i} \\
&p^{(t)} = \frac{1}{m}\sum_{i = 1}^{m}p^{(t)}_{i} 
\end{aligned} 
\end{equation}

In the end, a product has three types of information representation, including global feature representation $p^{(t)}$, local feature representation $l^{(t)}$, and category representation $v^{c}$. I concatenate all these information to calculate the estimated product sentiment distribution $d^{(t)}$ over all pre-selected aspects via one layer MLP normalized by $\rm softmax(\cdot)$ function:
\begin{equation}
d^{(t)} = {\rm softmax}(W^{d}[p^{(t)},l^{(t)},v^{c}]+b^{d})
\end{equation}
where $W^{d}$ and $b^{d}$ are related weight matrix and bias. $[\,, ]$ denotes the concatenation operation. 

This pretraining process is optimized by minimizing the cross entropy between the estimated product sentiment distribution $d^{(t)}$ and the actual product sentiment distribution calculated from product reviews (Section \ref{sc:dp}). This optimization step has to be done ahead of the main multi-task model introduced in the following sections.



\subsubsection{Reinforcement Neighbour Selection} \label{sc:rns}

In the $t_{th}$ time period, among $h$ seed products with sufficient reviews, a policy gradient approach earns an action to select $s$ neighbour products $\mathcal{A} = \{\alpha_{1}^{(t)},\alpha_{2}^{(t)},...,\alpha_{s}^{(t)}\}$ out of $h$ candidates where $\alpha_{s}^{(t)}$ denotes the $s_{th}$ selected neighbour product. As I do not consider the sampling sequence on neighbour products, the reinforcement approach is a one-step Markov Decision Process (MDP) with single state $\mathcal{S}$ and single action $\mathcal{A}$. 

Assume there are $n$ products in total across all time periods, given a target product, the initial observation $\mathcal{O}$ is the product itself, represented as a one-hot embedding $\in \mathbb{R}^{n}$. The state $\mathcal{S}$ is learned via a two-layer Multilayer Perception (MLP) on the initial observation $\mathcal{O}$:
\begin{equation}
\label{eq:state}
\mathcal{S} = {\rm softmax} (W_{2}{\rm tahn}(W_{1}\mathcal{O} + b_{1}))
\end{equation} 
where $W_1$,$W_2$ and $b_1$ denote related weight matrices and bias, respectively. 

The learned state $\mathcal{S} \in \mathbb{R}^{h}$ is the selection probability distribution over $h$ seed products. Assuming an action is taken to sample $s$ neighbour products with related probability weights $\{\omega_{1}^{(t)},\omega_{2}^{(t)},...,\omega_{s}^{(t)}\} \in \mathcal{S}$, the sampling policy $\pi_{\Theta_{1}}(\mathcal{A}|\mathcal{S})$ can be therefore calculated as:
\begin{equation}
\pi_{\Theta_{1}}(\mathcal{A}|\mathcal{S}) = s!\prod_{i=1}^{s}\omega^{(t)}_{i}
\end{equation} 
$\Theta_{1}$ denote the parameters to be learned. The factorial of $s$ ($s!$) denotes the number of permutations for the selected neighbour products as the neighbour products are sequence insensitive. $\prod_{i=1}^{s}\omega^{(t)}_{i}$ is the generative probability of each permutation. 

To assess the fitness of the $s$ selected neighbour products for the target product, U design two dynamic rewards: a sentiment reward to measure the aspect-sentiments similarity, and a semantic reward to calculate the content similarity between $s$ neighbour products and the target product.

\textbf{Sentiment Reward:} 
In the $t_{th}$ time period, U estimate the target product's sentiment distribution as $d^{(t)}_{a}$ by the Sentiment Prediction Pretraining (Section \ref{sc:spp}). The sentiment distributions of all its selected neighbour products $\{d^{(t)}_{1},...,d^{(t)}_{s}\}$ are calculated directly from their reviews (Section \ref{sc:dp}). To evaluate pairwise distribution similarity, Pearson correlation calculates the sentiment reward $ \mathcal{R}_{sen,i}^{(t)}$ of the $i_{th}$ selected neighbour product $\alpha_{i}^{(t)}$ as follows:

\begin{equation}
\mathcal{R}_{sen,i}^{(t)} = \frac{\mathbb{E}[(d_{a}^{(t)}-\mu(d_{a}^{(t)}))(d_{i}^{(t)}-\mu(d_{i}^{(t)}))]}{\sigma(d_{a}^{(t)})\sigma(d_{i}^{(t)})}, i = 1,...,s
\end{equation}

where $\mathbb{E}(\cdot)$ denotes the expectation, $\mu(\cdot)$ denotes the mean and $\sigma(\cdot)$ denotes the standard deviation. Larger similarity score offers a higher reward to the related neighbour product.

\textbf{Semantic Reward:} In the $t_{th}$ time period, the semantic reward of neighbour products is measured by the accuracy of generated product summarization. In this paper, we use the word level Jaccard Similarity as the indicator to calculate the semantic reward $\mathcal{R}_{sem}^{(t)}$ for all selected neighbour products, which contains two parts: the averaged Jaccard Similarity between all neighbour product original reviews and real product summarization, and the Jaccard Similarity between generated product summarization and actual product summarization:

\begin{equation}
\mathcal{R}_{sem}^{(t)} = \frac{1}{s}\sum_{i=1}^{s} \frac{|R_{i}^{(t)} \cap Y^{(t)}|}{|R_{i}^{(t)} \cup Y^{(t)}|} + \frac{|\hat{Y}^{(t)} \cap Y^{(t)}|}{|\hat{Y}^{(t)} \cup Y^{(t)}|} 
\end{equation}

where $R_{i}^{(t)}$ denotes all original reviews of the $i_{th}$ neighbour product $\alpha_{i}^{(t)}$, $\hat{Y}^{(t)}$ denotes the generated summarization of target product, and $Y^{(t)}$ denotes its actual summarization. The total reward $\mathcal{R}^{(t)}_{\mathcal{A}}$ of neighbour products is the weighted sum between sentiment reward and semantic reward controlled by a weighting factor $\gamma$:


\begin{equation}
\mathcal{R}^{(t)}_{\mathcal{A}} = \sum_{i=1}^{s} \omega_{i}^{(t)}\mathcal{R}_{sen,i}^{(t)} +\gamma \mathcal{R}_{sem}^{(t)}
\end{equation}

\textbf{Task Optimization:} I use policy gradient method to optimize the sampling policy, aiming to maximize the expected total reward for neighbour products. The expected reward $\mathcal{J}_{sel}(\Theta_{1})$ in the $t_{th}$ time period is:

\begin{equation}
\begin{aligned}
\mathcal{J}_{sel}(\Theta_{1}) &= \mathbb{E}_{\mathcal{A} \sim \pi_{\Theta_{1}}(\mathcal{A}|\mathcal{S})}[\mathcal{R}^{(t)}_{\mathcal{A}}]
\end{aligned}
\end{equation}

Then, the gradient is estimated using the likelihood ratio trick:

\begin{equation}
\label{eq:loss_1}
\begin{aligned}
\nabla_{\Theta_{1}}\mathcal{J}_{sel}(\Theta_{1}) &= \nabla_{\Theta_{1}} \sum_{\mathcal{A}} \pi_{\Theta_{1}}(\mathcal{A}|\mathcal{S}) \mathcal{R}^{(t)}_{\mathcal{A}}\\
&\approx \frac{1}{N} \sum_{i=1}^{N}\nabla_{\Theta_{1}}log\pi_{\Theta_{1}}(\mathcal{A}_{i}|\mathcal{S})\mathcal{R}^{(t)}_{\mathcal{A}_{i}}
\end{aligned}
\end{equation}

where $\mathcal{A}_{i}$ denotes the $i_{th}$ of $N$ randomly sampled actions (selecting $s$ neighbour products from $h$ seed products). 

\subsection{Summarization Generation Task} \label{sc:sgt}

In the $t_{th}$ time period, the filtered neighbour product sentimental phrases together with product own descriptive phrases are concatenated into a sequence $X^{(t)} = \{x_{1},...,x_{w}\}$. It is used as the input of Neural Machine Translation (NMT) model to generate aspect summarization sequence $Y^{(t)} = \{y_{1},...,y_{k}\}$. $w$ and $k$ denote the input and output sequence length, respectively. Filtering out most of emotional and other irrelevant words in advance can better map the input to aspect oriented summarizations instead of subjective review summaries. 

The input sequence $X^{(t)} = \{x_{1},...,x_{w}\}$ is fed one-by-one into the encoder (a single-layer bidirectional LSTM), producing a sequence of encoder hidden states $\{e_{1},...,e_{w}\}$. In decoding step $i$, the decoder (a single-layer unidirectional LSTM) has a decoder hidden state $h_{i}$. Its context vector $u_{i}$ is generated via an Attention mechanism on all encoder hidden states and current decoder hidden state:

\begin{equation}
\begin{aligned}
& a_{ij} = {\rm attention}(h_{i},e_{j}), j = 1,...,w \\
& a_{ij}^{*} = \frac{{\rm exp}(a_{ij})}{\sum_{k=1}^{w} {\rm exp}(a_{ik})}\\
& u_{i} = \sum_{j=1}^{w}a_{ij}^{*}e_{j}
\end{aligned}
\end{equation}
where $a_{ij}$ denotes the attention weight of encoder hidden state $e_{j}$. $a_{ij}^{*}$ is the normalized weight by Softmax function. The weighted sum of all encoder hidden states, $u_{i}$, is the context vector for current step $i$, reflecting the auxiliary information from input sequences.

A one-layer MLP is subsequently utilized on the combination of context vector $u_{i}$ and decoder hidden state $h_{i}$ to generate the vocabulary probability distribution $P_{vocab}$:

\begin{equation}
P_{vocab} = {\rm softmax}(W^{o}[u_{i},h_{i}]+b^{o})
\end{equation}
where $W^{o}$ and $b^{o}$ are related weight and bias.

In decoding step $i$, the generation loss for target word $y_{i}$ is its negative log likelihood, $-logP_{vocab}(y_{i})$. The overall generation loss $\mathcal{J}_{gen}(\Theta_{2})$ is the average of all $k$ step generation losses and $\Theta_{2}$ are all related parameters to be optimized.

\begin{equation}
\mathcal{J}_{gen}(\Theta_{2}) = \frac{1}{k}\sum_{i=1}^{k} -logP_{vocab}(y_{i})
\end{equation}

In the multi-task model, $\mathcal{J}_{sel}(\Theta_{1})$ and $\mathcal{J}_{gen}(\Theta_{2})$ both need to be minimized during the model training process. Each task is learned separately and alternately after taking a certain number of training data batches in their optimization processes.