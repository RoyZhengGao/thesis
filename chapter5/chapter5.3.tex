\section{Experiments} \label{sc:exp} 
\subsection{Dataset}
From \textit{Taobao}, a world-leading E-commerce website owned by Alibaba, we collect user behavior and product reviews during the period from Apr/12/2018 to Jul/10/2018.the raw dataset is split into two parts: the first two-week data, as the initial time period $t_0$, helps to generate user consistent shopping preference (\textit{Section \ref{sc:dpbr}}). For the rest data, we empirically set fifteen days as the time window to split it evenly into five consecutive time periods ($t_{1} \sim t_{5}$). In each time period, four types of information need to be prepared and calculated in advance to support model training, including multi-type user behavior, product profile, product sentiment distribution, and product ground truth summarization: 

\textbf{First}, multi-type user behavior is used for generating dynamic product behavior representation (\textit{Section \ref{sc:dpbr}}). Three types of user behavior are considered in this paper including `\textit{Click}', `\textit{Add Cart}' and `\textit{Purchase}'.

\textbf{Second}, from product profile, product category is encoded as one-hot embedding for sentiment distribution prediction (\textit{Section \ref{sc:npst}}). Descriptive phrases are extracted from product description profiles to support aspect summarization generation (\textit{Section \ref{sc:sgt}}). 

\textbf{Third}, product sentiment distribution is calculated from reviews (\textit{Section \ref{sc:dp}}) as the ground truth of sentiment prediction pretraining (\textit{Section \ref{sc:npst}}). We consider four common aspects including `\textit{Quality}', `\textit{Cost-performance Ratio}', `\textit{Fitness}' and `\textit{Material}'.

\textbf{Fourth}, in \textit{Taobao}, a user can rate reviews with thumbs-up or thumbs-down signal. For each product in the training data, we firstly select its top ten reviews relevant to the four picked aspects and with the largest number of thumbs-ups. After that, using AliNLP sentiment analyzer, we can locate and filter out the aspect-related sentences from the reviews as the ground truth for aspect summarization generation (\textit{Section \ref{sc:sgt}}).  

In total, the whole dataset contains 125,598 products, 51,366 users and 108,749,788 multi-type behaviors. We use 80\% of the data for training, 10\% for validation and 10\% for testing.  


\subsection{Baselines and Settings} \label{sc:baseline}

As the main contribution of this paper lies in the reinforcement neighbour product selection task, five baselines are chosen to compare from neighbour product selection viewpoint. \textbf{1) Title Similarity} (TS): Neighbour products are selected with the shortest Levenshtein Distance on titles. \textbf{2) Random}: Neighbour products are randomly selected. \textbf{3) PMF}: PMF \cite{mnih2008probabilistic}  firstly learns product embeddings via matrix factorization. Neighbour products are selected with the highest cosine similarity score on product embeddings. \textbf{4) GBPR}: GBPR \cite{pan2013gbpr} uses a Bayesian based collaborative filtering method to assign user preferences on products. Neighbour products are selected with the most similar user preferences. \textbf{5) EALS}: EALS \cite{he2016fast} is a fast matrix factorization approach which learns product embeddings. Neighbour products are selected with the highest cosine similarity score on product embeddings. After the neighbour products are selected, their reviews' sentimental phrases together with product own descriptive phrases are utilized to generate aspect summarization via the same NMT model struture  as our BDS model. We intentionally apply the same generative model so as to compare the effectiveness of neighbour products selection in different models.

To assess the usefulness of neighbour product information, our model is compared with another two generative models only leveraging product own  metadata. \textbf{6) Raw Title} (RT) uses the product titles and  \textbf{7) Title-Review} (TR) uses the concatenation of product title and sparse reviews as the input of the same NMT model to generate aspect summarization.
 

Mini-batch (\textit{size = 20}) Adam SGD optimizer is used to train our model for 100 epochs. Learning rate is 0.01. Dimension of product \& user representation is 300 (\textit{Section \ref{sc:dpbr}}). $s=5$ neighbour products are selected from $h=100$ seed products (\textit{Section \ref{sc:npst}}). Vocabulary size in summarization generation (\textit{Section \ref{sc:sgt}}) is \textit{15K}. Other parameter details will be offered once the paper gets published.


\subsection{Evaluation Metrics}
In this paper, we report the model performance via the following metrics from both automatic and human perspectives. Automatic metrics evaluate the accuracy of the generated summarizations by objectively calculating the correctness of predicted words. While human metrics evaluate the semantic quality of generated summarizations by subjectively considering their sentence readability. 
\begin{itemize}
	\item \textbf{ROUGE} (RG)\footnote{\url{https://pypi.org/project/pyrouge/}}
	: evaluates text summarization quality by comparing the overlap between generated sequences and the ground truth. We report RG-1, RG-2 and RG-L in this paper.
	\item \textbf{METEOR}\footnote{\url{http://www.cs.cmu.edu/~alavie/METEOR/}}
	: is the harmonic mean of generated summarizations' unigram precision and recall. It offers stemming and synonymy matching along with standard exact word matching.
	
	\item \textbf{Human Evaluation}: We generate summaries of 200 random products by each of the seven baselines and our model. To compare BDS model with each baseline, three human votes are collected from a crowd-sourcing platform for each pair of product summaries ($200*7*3 = 4,200$ human judgements). People are required to vote the one with more comprehensive information and better sentence structure. We define \textit{winning time rate} (WTR) and \textit{winning count rate}  (WCR) as two human evaluation metrics. Given a pair of model $A$ and model $B$, the WTR of $A$ is the ratio of winning products for $A$ and the WCR of $A$ is the ratio of winning votes for $A$. For instance, given two products $\alpha$ and $\beta$, if method $A$ gets two votes for $\alpha$ and one vote for $\beta$, its WTR is $(1+0)/(1+1)=0.5$ and WCR is $(2+1)/(3+3)=0.5$.
\end{itemize}
