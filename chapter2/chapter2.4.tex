\section{Applications}
In this section, two types of applications are discussed from both interdisciplinary  and technical support perspectives.  For interdisciplinary supports, community detection can be applied in other domains to support better domain knowledge exploration. For technical supports, various public dataset and open-source toolkits are introduced for model comparison and evaluation. The following subsections will introduce each type of supports in detail.

\subsection{Interdisciplinary Supports}
Social media is a major relevant domain towards community detection as user connections can naturally construct social networks. Besides that, other domains such as biology, physics and neural science also involve community detection a lot to explore their object hidden connections. In the following paragraphs, social media researches are separately introduced at first, and the rest relevant researches are demonstrated together afterwards.

\subsubsection{Social Media}

\cite{papadopoulos2012community} introduces and compares a set of community detection models and provides five strategies for how to apply these models to support large scale social media analysis, including sampling techniques, local graph processing, iterative schemes, multi-level approaches and parallelization. These techniques are applied to five types of social media applications including topic detection, tag disambiguation, user profiling construction, photo clustering and event detection. 


In collaborative tagging (a.k.a. folksonomy) systems, tripartite graphs can be constructed from users, images and tags given user behaviors in the systems. By defining random walk probability values between different types of nodes, \cite{xie2014community} detects latent user communities in folksonomy graphs through a proposed Approximate Prototype Clustering (APC) method which is a similar process as K-means method. The tags of neighboring community users are ranked and selected to enrich a userâ€™s profile. 

Due to the fact that some users create multiple sockpuppets to deceive other users or manipulate topics in online communities, \cite{kumar2017army} studies sockpuppetry to show the behavior difference between sockpuppets and ordinary users and thereafter detect sockpuppets to maintain the correct order of online communities. By analyzing 9 online communities, the study demonstrates particlar sockpuppets writing patterns (more singular first-person pronouns), write shorter sentences, and swear more) and behaving patterns (participate more controversial topics and more interact with other sockpuppets). In the end it builds up the taxonomy to differentiate and kick out sockpuppets for improving the quality of online discussion.


\cite{danescu2013no} studies individual user lifecycle evolving trend and linguistic change in online communities. In the analysis of its proposed framework, during the early stage (about one third of eventual lifespan) of individual users, they will tightly and increasing follow and be affiliated with current communities. However, after reaching to the peak point, a gap between the users'  language and community forms increases until finally they abandon the site. The main reason is because of linguistic changes. Language norms used in a community is evolving over time, and it would be harder and harder for senior users to accept this change. Once users feel themselves not able to cease the linguistic change, they will stay away from the community and eventually leave out.  Instead of analyzing individual users in online communities, \cite{kairam2012life} focuses on the lifecycle of online community itself. It finds out two types of community growth including diffusion growth,  where new members come in because of connections with existing members, and non-diffusion growth where new members come in without previous connections to the communities. It also builds up a model to predict community longivity and eventual size using graph structural features and past growth experiences. 

Users in social media such as Facebook and Twitter needs to identify their social circles either manually, or in a naive fashion by shared attributes, which is not enough to satisfy users need and lack of accuracy.  \cite{leskovec2012learning}  proposes an ego network model to define user social circles based on different aspects (i.e. family members or schoolmates) at first. Then it assigns user followings/followers into different social circles based on their profiles. It allows overlapping communities where a user can belong to multiple types social circles to current user.


Many of existing works jointly model textual content and user activities in social networks to enhance the appropriateness of detected communities. \cite{gargi2011large} presents a step-by-step approach in YouTube graphs by clustering videos into named clusters having associated tags and descriptions. It prepossess the YouTube graph and selects seed nodes using a greedy  method. After that, in order to enable community detection on millions of videos efficiently, it takes advantages of MapReduce to cluster videos in a parallel fashion. In the end, a post-processing approach is applied to refine and merge clusters by maximizing text coherence and minimizing community overlap. Having a similar goal to detect topic oriented communities, \cite{zhao2012topic} extracts social objects from emails and blogs. Based on their content, social objects are clustered into different topics. Users whose behaviors are involved in the same topic are grouped into same communities. \cite{sachan2012using} also aims to detect topically meaningful communities by considering both graph topological structure and social content in a united way. Inspired by topic modeling, it proposes a generative Bayesian model which is named as Topic User Community Model (TUCM). In the model, it assumes community membership is dependent on both topic interests from posts and graph structure. The whole learning process is guided by a Markov Chain Monte Carlo (MCMC) sampling strategy. In the end, a person can belong to multiple communities and each community can contain multiple topics. For the same purpose,  \cite{natarajan2013community} proposes an improved version of probabilistic model to jointly detect user communities and content topics by leveraging both their social connections and shared content in Twitter. In its model, community is treated as an affiliation probability distribution on users. User connections as well as associated communities topics are generated from community structure by utilizing Gibbs Sampling. Instead of using topic modeling, \cite{ozer2016community} addresses Non-negative matrix factorization method which leverages social connectivity and content information for community detection. Besides user connectivity, It considers three types of content information including word, hashtag and domain as auxiliary terms to regularize the matrix factorization process. Its empirical experiments show that word usage is the strongest indicator of user potential community affiliation among all three types of content  information.  

Mobile devices have more and more become an essential part of our daily life with the proliferation of wireless technologies. Typically, a mobile social network (MSN) is constructed by user call logs. \cite{wang2010community} introduces a Community-based Greedy algorithm (CGA) model which contains two major components including a community detection model to take care of information diffusion, and a dynamic programming model to select top $K$ most influential users given the community partition. \cite{botta2017analysis} studies on evolutionary MSN over time, reveals similar circadian and weekly patterns happened in both individual user level and community level.


\cite{traud2011comparing} studies the graphs of Facebook ``friendships'' at five U.S. universities. It investigates the community structure of each single university graph and employs visualization and quantitative analysis to measure the correlation between user communities across universities. After examining the community impact of a set of self-identified user characteristics such as residence, class year, major, and high school in all university graphs, the study concludes that student relationships are organized and dominated by multiple key factors instead of a single one. 

\subsubsection{Miscellaneous Domains}
\cite{garcia2018applications} demonstrates how modularity method detects node communities within brain graphs to reveal human neural systems functioning on the main healthy human cognition. In this study, the defined nodes in a brain graph can be flexible but need to implicate network dynamics, such as brain regions or neurons.  A d edges can reflect structural connections across spatial scales, such as bundles of axonal fibers between regions or synapses between neurons.  Similarly, \cite{liu2014network} also presents how community detection methods employed as neuroscience applications to identify the functional brain modules from multichannel and multiple subject neuroimaging data. It proposes a refined model based on modularity to detect communities in effective brain connectivity graphs. The paper quantifies brain connectivity with a defined directed information (DI) metric. It thereafter extends the Louvain method in a group of brain graphs to detect the most involved functional electrode modules in cognitive control.

Community detection techniques also contribute a lot in the biology domain to discover the hidden modules and potential bindings between proteins. \cite{he2016evolutionary} proposes a EGCPI model to identify protein complexes in the detected clusters from protein-protein interaction graphs. Based on the public Gene Ontology (GO) database, this paper constructs a protein attribute graph. With an evolutionary strategy to maximize the Independence of Cluster (IoC) fitness function, protein clusters are achieved through a genetic framework. In the end, a breadth first search (BFS) approach is applied to select sub-graphs that consist of similar proteins in each cluster based on their degree of attribute homogeneity. Having a similar propose, ClusterONE  \cite{nepusz2012detecting} is step-by-step approach to detect overlapping protein complexes from protein-protein interaction graphs. Specifically, it firstly utilizes a greedy approach to group proteins with high cohesiveness. Secondly, a merging process is taken on pairwise raw protein clusters to merge those  clusters with high predefined overlap score. At last, it leaves out those small clusters with trivial influence in graphs. \cite{lewis2010function} also reaches a conclusion that community structure in protein interaction graphs can benefit biological findings by probing different scales/resolutions in the graphs.

\cite{gupta2011evolutionary} proposes a generative model to detect communities in the DBLP dataset, which is the largest computer science bibliography database. By detecting communities in a  constructed heterogeneous bibliographical graph involving author, paper, conference and particular word/term nodes, it calculates the continue, merge and split rates of words/terms as well as authors in the graphs over time. It also shows how evolutionary communities are appeared and disappeared as well. Another work, OverCite \cite{chakraborty2013overcite}, also applies community detection in bibiliometrics, which aims to detect overlapping communities of authors, papers and venues simultaneously through the graphs constructed by these types of nodes. After detecting all node communities, the study builds up a recommendation system to use the overlapping communities as recommendation results to users.

\cite{hu2016co} proposes a CENFLD model to deal with community detection in business/enterprise graphs. A business graph involves user nodes to represent  producers, suppliers, and customers with side textual information such as recruiting messages or advertisements. To deal with business graphs' intrinsic nature of diversity, inconsistency, Implicitly and richness, the paper proposes a co-clustering factorization based approach to detect user groups. Specifically, it employs a nonnegative matrix factorization method to factorize the graph topological and textual information in individual forms (including node-feature content matrix factorization, network topology structure matrix factorization and feature-feature correlation matrix factorization). After that, it proposes a consensus principle to optimize these forms jointly. 

Besides aforementioned applications, community detection can also be applied in the chemical domain to discover force-chain clusters in granular materials \cite{bassett2015extraction}, music domain to extract musical rhythmic pattern \cite{coca2016musical}, and question-answer system \cite{fang2016community} to improve Q\&A matching accuracy.  

\subsection{Technical Supports}

After understanding how those state-of-the-art models work, a subsequent task for researchers is how to employ these models on various graphs. As most of models are too complex to be implement by individual researcher within a short period of time, open-source softwares and packages are required with an urgent need. Similarly,  benchmark graphs are also an essential part of model evaluation as they enable to testify different models under a fair condition.  In the following paragraphs, I will list and detailedly introduce several publicly available graph repositories, widely used softwares and programming toolkits. All of them significantly ease and contribute to the evaluation process by offering an environment to compare model performances under different graph scenarios.

\subsubsection{Datasets}
SNAP \cite{leskovec2015snap} is one of the most famous graph repository which contains hundreds of graph datasets collected by Stanford University\footnote{http://snap.stanford.edu/data/}. Within the graph repository, there are graphs with different semantic meanings such as social networks, citation graphs, communication graphs and web graphs. Regarding to the graph types, it includes signed graphs, temporal graphs and attribute graphs, etc. In total, the repository contains over 70 graph datasets which size range from thousands nodes to millions nodes. Many community detection papers published by Stanford University reply on this repository.

KONECT \cite{kunegis2013konect}  is an open project held by the University of Koblenzâ€“Landau, which particularly collects large graph datasets for academic research\footnote{http://konect.uni-koblenz.de/networks/}. Up until now, there are 261 graphs with various types including (un)directed, (un)weighted, and signed graphs, etc. These graphs are also associated with different semantics such as social networks, citation graphs and communication graphs. 

LAW graph repository\footnote{http://law.di.unimi.it/datasets.php} is owned and managed by the University of Milan, which particularly focuses on big graph storage. In its repository, there are around 80 big graphs compressed via WebGraph, a graph compression framework, to  enable their quick downloading. There are various types of graphs such as web graphs, social graphs in the repository, each of which contains up to 100 million nodes and over 3 billion edges.  Even though the repository official website is still maintained by the host, after the year 2012, there is no major updates in either graph datasets or relevant published papers.

NR \cite{rossi2015network} is an interactive graph repository which enables graph downloading and interactive web analysis through their web-based platform\footnote{http://networkrepository.com/networks.php}. Currently, NR contains over 6000 graphs from 19 general domains, i.e. social netoworks, biological graphs. It covers a wide range of graph types such as bipartite graphs and temporal graphs as well. With the help of their interactive web interface, researchers can easily discover these graphs without too much effort. The lightweight platform also allows researchers to upload and share their own graphs. In this repository, the size of graph nodes is from hundreds to over 10 million. And the size of graph edges is from hundreds to over 100 million. 

There is a repository of Benchmark Graph Datasets in Github\footnote{https://github.com/shiruipan/graph\_datasets}, in which there are 31 medium-size graph datasets (each graph contains up to 10 thousand nodes). The types of graph include biological graphs, citation graphs, social networks and brain graphs. According to the repository, a series of models are assessed by graph classification and community detection evaluation tasks using these graph datasets.

\subsubsection{Softwares}

Pajek\footnote{http://mrvar.fdv.uni-lj.si/pajek/} is a public network analysis  toolkit particularly for analysis and visualization on very large graphs. It is a well-maintained nonprofit project and the latest version is release in September, 2019. This tool offers hands-on manuals in all major languages including English, Chinese and Spanish. Originally, this software is deigned to run on Windows system. However, in recent years, several extensions are built up to enable running Pajek in both Mac and Linux system as well. Pajek is one of the best known graph analysis toolkit. The eligible graph format created by Pajek even becomes a standard format of graph data. Besides visualizing graphs into a 2-D plot, it can also calculate several major graph metrics such as triangle numbers and graph modularity score.  Several basic community detection, such as Louvian method, are also embedded in Pajek to reveal high order graph structure.

NetMiner\footnote{http://www.netminer.com/main/main-read.do} is a commercial software for exploratory analysis and visualization on graphs. It is owned by a Korean company named CYRAM. The software is similar to Pajek software but has more interactive functions. As one of its advantages, It supports large network analysis on graphs with thousands of nodes, which can satisfy most of needs in the social science domain. Moreover, five community detection methods, including Modularity method and label propagation method, are implemented in the software to support more intricate graph analysis.  

CFinder\footnote{http://www.cfinder.org/} is a free graphic tool to find graph communities and visualize the generated node cliques in 2-D plots. It is developed based on Java Spring framework and has a similar layout to other previously mentioned softwares. It requires to install Java Runtime Environment (JRE) as a prerequisite. CFinder is able to run under all main platforms (i.e. Windowns, Mac and Linux ) and can be easily installed by researchers with no technical background. The latest version is released in the year 2014 and the latest paper is published in 2016. To summary, it is a possible software option for researchers to explore and visualize graph communities but the back-end support is a little bit out of date.  

Gephi\footnote{https://gephi.org/} is a leading visualization and exploration tool for social networks. The goal of Gephi is to make better analysis to find patterns to uncover graph structures and visualize the result in an intuitive way, which makes it easier for users to understand. In its interface, there is a visualization window along with a bunch of functional buttons. By clicking on the buttons or change the values of the related boxes, it can directly change the visualization layout. Even though Gephi offers a Python library as well, its software is still more popular and better accepted by users. One advantage of the software is that dynamic networks can be also conveniently explored. Another advanced feature is that Gephi is able to render 3-D plots in the visualization window. The recent major update is in 2017, which is able to support all types of platforms including Windows, Mac and Linux.

UCINET\footnote{https://sites.google.com/site/ucinetsoftware/home} is a software funded by a start-up company named Analytic Technologies. It is particularly designed for Windows system and needs to be purchased after 90 days free trial. This software is pretty active and the latest version is release in March 2020. Although the official website claims the software can handle graphs with up to 30 thousand nodes. However, empirical experiences show that its process will run slow when the graph size is over five thousand nodes. 

GUESS\footnote{http://graphexploration.cond.org/} is an exploratory data analysis and visualization tool for graphs. Claimed in its official website, the software contains a  script language called Gython to handle large graph visualization via Java Applet. There are also several community detection methods embedded in the software, which can help to visualize graph hidden structure. However, as there is no major update since 2007, researchers have abandoned this software. Even though,  it still offer some  insights for graph analysis. Unlike other softwares only need users to click buttons in the interactive interface,  GUESS requires users  to type commands instead for generating the graph plots. This feature raises the bar to learn this software. 

ORA-LITE\footnote{http://www.casos.cs.cmu.edu/projects/ora/} is a tookit for dynamic graph assessment, which is developed by the Carnegie Mellon University . It is a software mainly used for dynamic networks, which means the graphs they analyze change over time. As it claims in the official website, this software can not only find out the community structure of  dynamic graphs but also other graph metrics. Although  the time complexity is usually high in dynamic network analysis, this software is able to handle graphs with millions nodes. Another advantage of this software is that it offers multi-language tutorials and even holds Google groups for users to communicate. The latest version of this software is published in January 2020. But it only support to run on Windows system.

Cytoscape\footnote{http://www.cytoscape.org/} is an open-source platform originally designed for biological researches. Through the development in recent years, it currently turns to be a general platform for all types of graph analysis and visualization. Cytoscape offers a Javascript API to allow rendering graph plots in web applications and a Java API to allow connecting to remote servers.  Inside its software, there are a set of basic features installed by default. Beyond that, users have a choice to enable advanced features by installing plugins by themselves. The latest version of this software is released in June 2019. And based on the announcement from its official website, the number of software daily downloads is huge and its discussion forum is pretty active until now.

MuxViz\footnote{http://muxviz.net/index.php} is a framework for the multilayer graph analysis and visualization. It allows interactive visualization and exploration  for graphs with multi-type relationships and attributes, which is the most outstanding feature of this software. MuxViz is developed based on R and GNU Octave, which means it requires to install R in advance and can only deal with at most middle-size graphs. As a open-source software, it can run on Windows, Linux and Mac OS X. However, its latest release is in 2015, which is a bit out of date. 

Visone\footnote{https://visone.info/} is a free toolkit for analyzing and visualizing social networks, which is a long term project managed by the University of Konstanz. It can run either from terminal or through its interactive interface. The software supports all types of platforms including Windows, Mac and Linux. As It is originally written in Java, the current version released in 2019 requires at least Java 8 to be installed in advance. Many community detection methods are embedded in it such as Modularity and spectral methods. Another feature is that the rendered plots can be directly exported and saved to local disk. 

\subsubsection{Programming Toolkits}

As one of the most popular programming language in statistics, R contains many packages which are particularly designed for graph analysis and community detection. Wrapped as a high level programming language, R is not able to handle large scale graphs as other languages such as Java and C++. However thanks to its simplified design, it is able to support more complex models than other languages. In the following paragraphs, I will briefly cover three widely used packages including modMax, networktools and RANN.

modMax\footnote{https://cran.r-project.org/web/packages/modMax/modMax.pdf} is a R packages which contains a lot of Modularity based methods. As one of the largest methodological track in community detection, Modularity has a lot of variants and optimization solutions. In modMax, many variants are covered such as Modularity optimization using fast greedy, simulated annealing, spectral and  genetic methods.

networktools\footnote{https://cran.r-project.org/web/packages/networktools/networktools.pdf} is another newly developed R package particularly for graph analysis. Its functions include not only basic community detection algorithms but also have visualization functions. 

RANN\footnote{ https://github.com/jefferis/RANN} provides a set of fast $k$ nearest neighbor search models. It is a R wrapper of ANN library, which is originally written in C++.

igraph\footnote{https://igraph.org/} is one of the most widely used packages for community detection. It is originally written in C++ but offers R, Python and Mathematica wrappers. igraph package is frequently updated and the recent release is released in March 2020. There are a bunch of widely used community detection models implemented in the package such as Modularity, Walktrap, Label Propagation and Infomap method.


NetworkX\footnote{https://networkx.github.io/} is a fundamental Python package which defines the basic data structures used for graph analysis. Most of Python packages related to graph analysis are inherited from NetworkX. Within the package, it also contains several components such as centrality, clique, clustering in which several functions are related to community detection models or graph evaluation metrics.

SNAP\footnote{http://snap.stanford.edu/index.html} is a general graph mining library. It is written in C++ and easily scales to massive networks with hundreds of millions of nodes. The package has both C++ version and a Python wrapper. So far, it is the quickest package for community detection in large scale graphs. Another advantage of this package is that its embedded models keep updating frequently. It contains tens of state-of-the-art models for overlapping community, dynamic graph community, and Modularity based community detection. 

CDlib\footnote{https://cdlib.readthedocs.io/}  is a newly announced Python library for complex network analysis. The recent version is released in February 2020. This software is extended from NetworkX but offers more community detection methods and  evaluation metrics. Overall, It contains over 40 community detection methods about node clustering, edge clustering and overlapping community detection. Besides, it also offers functions to calculate more than 20 widely used evaluation metrics.

\subsection{Summary}
In this section, I firstly introduce how community detection can support researches in other domains such as citation recommendation, biology exploration and social media analysis. After that, a bunch of public resources, such as datasets and toolkits, are introduced to support community detection model evaluation. Softwares are easier to learn but programming toolkits have more flexibility to deal with complicated scenarios. Here are some tips and hints about how to choose toolkit if needed: 

First, toolkit selection is purpose oriented.  Researchers need to clarify their research tasks, and thereafter to select the proper toolkit. For example, if researchers need to analyse dynamic graphs, 
ORA-LITE is definitely the first choice to consider. 

Second, identify the preference of flexibility or simplicity. If researchers are lack of technical background or don't want to spend too much time for learning, then softwares should be a better choice than programming toolkits. While if researchers want more customized functions, programming toolkits will be a better choice. 

Third, price matters as well. Some paid softwares may contain more features than free ones. Researchers have to make sure whether it is worthy to invest on these commercial softwares such as UCINET.

Fourth, keep updated. There are many toolkit came out each year. Researchers need to keep up with the latest toolkit and explore the possibility to apply it into personal researches.  Usually toolkits released by high-tech companies and top tier universities are associated with enriched tutorials. They are easier for new users to start learning and more reliable to use compared with other open source packages shared via Github repository.

Overall, researchers need to consider all above mentioned tips before they choose datasets and softwares to use in their academic researches. The appropriate selection may significantly alleviate their workloads in model performance analysis.

